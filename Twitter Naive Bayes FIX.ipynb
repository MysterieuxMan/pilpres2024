{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAE7WPkQINAz"
      },
      "source": [
        "# Scrapping  Twitter Pemilu 2024 using tweet-harvers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Gathers Twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\asus.laptop-p9tbk6ts.000\\anaconda3\\lib\\site-packages (2.4.1)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: filelock in c:\\users\\asus.laptop-p9tbk6ts.000\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus.laptop-p9tbk6ts.000\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\asus.laptop-p9tbk6ts.000\\anaconda3\\lib\\site-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\asus.laptop-p9tbk6ts.000\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\asus.laptop-p9tbk6ts.000\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\asus.laptop-p9tbk6ts.000\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus.laptop-p9tbk6ts.000\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus.laptop-p9tbk6ts.000\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Local\\Temp\\ipykernel_36196\\1227983068.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installed Python packages:\n",
            "absl-py==2.0.0\n",
            "aiobotocore==2.12.3\n",
            "aiohappyeyeballs==2.4.0\n",
            "aiohttp==3.10.5\n",
            "aioitertools==0.7.1\n",
            "aiosignal==1.2.0\n",
            "alabaster==0.7.16\n",
            "altair==5.0.1\n",
            "anaconda-anon-usage==0.4.4\n",
            "anaconda-catalogs==0.2.0\n",
            "anaconda-client==1.12.3\n",
            "anaconda-cloud-auth==0.5.1\n",
            "anaconda-navigator==2.6.2\n",
            "anaconda-project==0.11.1\n",
            "annotated-types==0.6.0\n",
            "anyio==4.2.0\n",
            "appdirs==1.4.4\n",
            "archspec==0.2.3\n",
            "argon2-cffi-bindings==21.2.0\n",
            "argon2-cffi==21.3.0\n",
            "arrow==1.2.3\n",
            "astroid==2.14.2\n",
            "astropy-iers-data==0.2024.6.3.0.31.14\n",
            "astropy==6.1.0\n",
            "asttokens==2.4.1\n",
            "astunparse==1.6.3\n",
            "async-lru==2.0.4\n",
            "atomicwrites==1.4.0\n",
            "attrs==23.1.0\n",
            "autocommand==2.2.2\n",
            "automat==20.2.0\n",
            "autopep8==2.0.4\n",
            "babel==2.11.0\n",
            "backcall==0.2.0\n",
            "backports.functools-lru-cache==1.6.4\n",
            "backports.tarfile==1.2.0\n",
            "backports.tempfile==1.0\n",
            "backports.weakref==1.0.post1\n",
            "bcrypt==3.2.0\n",
            "beautifulsoup4==4.12.3\n",
            "binaryornot==0.4.4\n",
            "black==24.4.2\n",
            "bleach==4.1.0\n",
            "blinker==1.6.2\n",
            "bokeh==3.4.1\n",
            "boltons==23.0.0\n",
            "botocore==1.34.69\n",
            "bottleneck==1.3.7\n",
            "brotli==1.0.9\n",
            "cachetools==5.3.2\n",
            "certifi==2024.8.30\n",
            "cffi==1.16.0\n",
            "chardet==4.0.0\n",
            "charset-normalizer==3.3.2\n",
            "click==8.1.7\n",
            "cloudpickle==3.0.0\n",
            "colorama==0.4.6\n",
            "colorcet==3.1.0\n",
            "comm==0.2.2\n",
            "conda-build==24.7.1\n",
            "conda-content-trust==0.2.0\n",
            "conda-index==0.5.0\n",
            "conda-libmamba-solver==24.7.0\n",
            "conda-pack==0.7.1\n",
            "conda-package-handling==2.3.0\n",
            "conda-package-streaming==0.10.0\n",
            "conda-repo-cli==1.0.88\n",
            "conda-token==0.5.0+1.g2209e04\n",
            "conda-verify==3.4.2\n",
            "conda==24.7.1\n",
            "constantly==23.10.4\n",
            "contourpy==1.2.0\n",
            "cookiecutter==2.6.0\n",
            "cryptography==43.0.0\n",
            "cssselect==1.2.0\n",
            "cycler==0.11.0\n",
            "cytoolz==0.12.2\n",
            "dask-expr==1.1.13\n",
            "dask==2024.8.2\n",
            "datashader==0.16.3\n",
            "debugpy==1.8.5\n",
            "decorator==5.1.1\n",
            "defusedxml==0.7.1\n",
            "diff-match-patch==20200713\n",
            "dill==0.3.8\n",
            "distributed==2024.8.2\n",
            "distro==1.9.0\n",
            "docstring-to-markdown==0.11\n",
            "docutils==0.18.1\n",
            "entrypoints==0.4\n",
            "et-xmlfile==1.1.0\n",
            "executing==2.1.0\n",
            "fastjsonschema==2.16.2\n",
            "filelock==3.13.1\n",
            "flake8==7.0.0\n",
            "flask==3.0.3\n",
            "flatbuffers==24.3.25\n",
            "fonttools==4.51.0\n",
            "frozendict==2.4.2\n",
            "frozenlist==1.4.0\n",
            "fsspec==2024.6.1\n",
            "future==0.18.3\n",
            "gast==0.5.4\n",
            "gensim==4.3.3\n",
            "gitdb==4.0.7\n",
            "gitpython==3.1.43\n",
            "gmpy2==2.1.2\n",
            "google-auth-oauthlib==1.0.0\n",
            "google-auth==2.23.4\n",
            "google-pasta==0.2.0\n",
            "greenlet==3.0.1\n",
            "grpcio==1.59.2\n",
            "h2o==3.44.0.3\n",
            "h5py==3.11.0\n",
            "heapdict==1.0.1\n",
            "holoviews==1.19.1\n",
            "hvplot==0.10.0\n",
            "hyperlink==21.0.0\n",
            "idna==3.7\n",
            "imagecodecs==2023.1.23\n",
            "imageio==2.33.1\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.12.3\n",
            "importlib-metadata==7.0.1\n",
            "importlib-resources==6.4.0\n",
            "incremental==22.10.0\n",
            "inflect==7.3.1\n",
            "inflection==0.5.1\n",
            "iniconfig==1.1.1\n",
            "intake==0.7.0\n",
            "intervaltree==3.1.0\n",
            "ipykernel==6.29.5\n",
            "ipython-genutils==0.2.0\n",
            "ipython==8.27.0\n",
            "ipywidgets==7.8.1\n",
            "isort==5.13.2\n",
            "itemadapter==0.3.0\n",
            "itemloaders==1.1.0\n",
            "itsdangerous==2.2.0\n",
            "jaraco.classes==3.2.1\n",
            "jaraco.context==5.3.0\n",
            "jaraco.functools==4.0.1\n",
            "jaraco.text==3.12.1\n",
            "jedi==0.19.1\n",
            "jellyfish==1.0.1\n",
            "jinja2==3.1.4\n",
            "jmespath==1.0.1\n",
            "joblib==1.4.2\n",
            "json5==0.9.6\n",
            "jsonpatch==1.33\n",
            "jsonpointer==2.1\n",
            "jsonschema-specifications==2023.7.1\n",
            "jsonschema==4.19.2\n",
            "jupyter-client==8.6.2\n",
            "jupyter-console==6.6.3\n",
            "jupyter-core==5.7.2\n",
            "jupyter-events==0.10.0\n",
            "jupyter-lsp==2.2.0\n",
            "jupyter-server-terminals==0.4.4\n",
            "jupyter-server==2.14.1\n",
            "jupyter==1.0.0\n",
            "jupyterlab-pygments==0.1.2\n",
            "jupyterlab-server==2.25.1\n",
            "jupyterlab-widgets==1.0.0\n",
            "jupyterlab==4.0.11\n",
            "keras==3.5.0\n",
            "keyring==24.3.1\n",
            "kiwisolver==1.4.4\n",
            "langdetect==1.0.9\n",
            "lazy-loader==0.4\n",
            "lazy-object-proxy==1.10.0\n",
            "lckr-jupyterlab-variableinspector==3.1.0\n",
            "libarchive-c==2.9\n",
            "libclang==16.0.6\n",
            "libmambapy==1.5.8\n",
            "linkify-it-py==2.0.0\n",
            "llvmlite==0.43.0\n",
            "lmdb==1.4.1\n",
            "locket==1.0.0\n",
            "lxml==5.2.1\n",
            "lz4==4.3.2\n",
            "markdown-it-py==2.2.0\n",
            "markdown==3.4.1\n",
            "markupsafe==2.1.3\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib==3.9.2\n",
            "mccabe==0.7.0\n",
            "mdit-py-plugins==0.3.0\n",
            "mdurl==0.1.0\n",
            "menuinst==2.1.2\n",
            "mistune==2.0.4\n",
            "mkl-fft==1.3.10\n",
            "mkl-random==1.2.7\n",
            "mkl-service==2.4.0\n",
            "ml-dtypes==0.4.1\n",
            "more-itertools==10.1.0\n",
            "mpmath==1.3.0\n",
            "msgpack==1.0.3\n",
            "multidict==6.0.4\n",
            "multipledispatch==0.6.0\n",
            "mypy-extensions==1.0.0\n",
            "mypy==1.10.0\n",
            "namex==0.0.8\n",
            "navigator-updater==0.5.1\n",
            "nbclient==0.8.0\n",
            "nbconvert==7.10.0\n",
            "nbformat==5.9.2\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.3\n",
            "nlp-id==0.1.15.0\n",
            "nltk==3.9.1\n",
            "notebook-shim==0.2.3\n",
            "notebook==7.0.8\n",
            "numba==0.60.0\n",
            "numexpr==2.8.7\n",
            "numpy==1.26.4\n",
            "numpydoc==1.7.0\n",
            "oauthlib==3.2.2\n",
            "openpyxl==3.1.5\n",
            "opt-einsum==3.3.0\n",
            "optree==0.12.1\n",
            "ordered-set==4.1.0\n",
            "overrides==7.4.0\n",
            "packaging==24.1\n",
            "pandas==2.2.2\n",
            "pandocfilters==1.5.0\n",
            "panel==1.4.4\n",
            "param==2.1.1\n",
            "paramiko==2.8.1\n",
            "parsel==1.8.1\n",
            "parso==0.8.4\n",
            "partd==1.4.1\n",
            "pathspec==0.10.3\n",
            "patsy==0.5.6\n",
            "pexpect==4.8.0\n",
            "pickleshare==0.7.5\n",
            "pillow==10.4.0\n",
            "pip==24.2\n",
            "pkce==1.0.3\n",
            "pkginfo==1.10.0\n",
            "platformdirs==4.3.2\n",
            "plotly==5.22.0\n",
            "pluggy==1.0.0\n",
            "ply==3.11\n",
            "prometheus-client==0.14.1\n",
            "prompt-toolkit==3.0.47\n",
            "protego==0.1.16\n",
            "protobuf==4.25.0\n",
            "psutil==6.0.0\n",
            "ptyprocess==0.7.0\n",
            "pure-eval==0.2.3\n",
            "py-cpuinfo==9.0.0\n",
            "pyarrow==16.1.0\n",
            "pyasn1-modules==0.2.8\n",
            "pyasn1==0.4.8\n",
            "pycodestyle==2.11.1\n",
            "pycosat==0.6.6\n",
            "pycparser==2.21\n",
            "pyct==0.5.0\n",
            "pycurl==7.45.3\n",
            "pydantic-core==2.20.1\n",
            "pydantic==2.8.2\n",
            "pydeck==0.8.0\n",
            "pydispatcher==2.0.5\n",
            "pydocstyle==6.3.0\n",
            "pyerfa==2.0.1.4\n",
            "pyflakes==3.2.0\n",
            "pygments==2.18.0\n",
            "pyjwt==2.8.0\n",
            "pylint-venv==3.0.3\n",
            "pylint==2.16.2\n",
            "pyls-spyder==0.4.0\n",
            "pynacl==1.5.0\n",
            "pyodbc==5.1.0\n",
            "pyopenssl==24.2.1\n",
            "pyparsing==3.1.2\n",
            "pyqt5-sip==12.13.0\n",
            "pyqt5==5.15.10\n",
            "pyqtwebengine==5.15.6\n",
            "pysocks==1.7.1\n",
            "pytest==7.3.1\n",
            "python-dateutil==2.9.0.post0\n",
            "python-dotenv==0.21.0\n",
            "python-json-logger==2.0.7\n",
            "python-lsp-black==2.0.0\n",
            "python-lsp-jsonrpc==1.1.2\n",
            "python-lsp-server==1.10.0\n",
            "python-slugify==5.0.2\n",
            "python-snappy==0.6.1\n",
            "pytoolconfig==1.2.6\n",
            "pytz==2024.1\n",
            "pyviz-comms==3.0.2\n",
            "pywavelets==1.5.0\n",
            "pywin32-ctypes==0.2.2\n",
            "pywin32==306\n",
            "pywinpty==2.0.10\n",
            "pyyaml==6.0.1\n",
            "pyzmq==26.2.0\n",
            "qdarkstyle==3.2.3\n",
            "qstylizer==0.2.2\n",
            "qtawesome==1.2.2\n",
            "qtconsole==5.5.1\n",
            "qtpy==2.4.1\n",
            "queuelib==1.6.2\n",
            "referencing==0.30.2\n",
            "regex==2024.7.24\n",
            "requests-file==1.5.1\n",
            "requests-oauthlib==1.3.1\n",
            "requests-toolbelt==1.0.0\n",
            "requests==2.32.3\n",
            "rfc3339-validator==0.1.4\n",
            "rfc3986-validator==0.1.1\n",
            "rich==13.7.1\n",
            "rope==1.12.0\n",
            "rpds-py==0.10.6\n",
            "rsa==4.9\n",
            "rtree==1.0.1\n",
            "ruamel-yaml-conda==0.17.21\n",
            "ruamel.yaml==0.17.21\n",
            "s3fs==2024.6.1\n",
            "sastrawi==1.0.1\n",
            "scikit-image==0.23.2\n",
            "scikit-learn==1.2.2\n",
            "scikit-posthocs==0.9.0\n",
            "scipy==1.13.1\n",
            "scrapy==2.11.1\n",
            "seaborn==0.13.2\n",
            "semver==3.0.2\n",
            "send2trash==1.8.2\n",
            "service-identity==18.1.0\n",
            "setuptools==72.1.0\n",
            "sip==6.7.12\n",
            "six==1.16.0\n",
            "smart-open==5.2.1\n",
            "smmap==4.0.0\n",
            "sniffio==1.3.0\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soupsieve==2.5\n",
            "sphinx==7.3.7\n",
            "sphinxcontrib-applehelp==1.0.2\n",
            "sphinxcontrib-devhelp==1.0.2\n",
            "sphinxcontrib-htmlhelp==2.0.0\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==1.0.3\n",
            "sphinxcontrib-serializinghtml==1.1.10\n",
            "spyder-kernels==2.5.0\n",
            "spyder==5.5.1\n",
            "sqlalchemy==2.0.30\n",
            "stack-data==0.6.3\n",
            "statsmodels==0.14.2\n",
            "streamlit==1.37.1\n",
            "sympy==1.13.2\n",
            "tables==3.10.1\n",
            "tabulate==0.9.0\n",
            "tblib==1.7.0\n",
            "tenacity==8.2.3\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorboard==2.17.1\n",
            "tensorflow-estimator==2.14.0\n",
            "tensorflow-hub==0.15.0\n",
            "tensorflow-intel==2.17.0\n",
            "tensorflow-io-gcs-filesystem==0.31.0\n",
            "tensorflow==2.17.0\n",
            "termcolor==2.3.0\n",
            "terminado==0.17.1\n",
            "text-unidecode==1.3\n",
            "textdistance==4.2.1\n",
            "threadpoolctl==3.5.0\n",
            "three-merge==0.1.1\n",
            "tifffile==2023.4.12\n",
            "tinycss2==1.2.1\n",
            "tldextract==5.1.2\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "tomlkit==0.11.1\n",
            "toolz==0.12.0\n",
            "torch==2.4.1\n",
            "tornado==6.4.1\n",
            "tqdm==4.66.5\n",
            "traitlets==5.14.3\n",
            "truststore==0.8.0\n",
            "twisted-iocpsupport==1.0.2\n",
            "twisted==23.10.0\n",
            "typeguard==4.3.0\n",
            "typing-extensions==4.12.2\n",
            "tzdata==2024.1\n",
            "uc-micro-py==1.0.1\n",
            "ujson==5.10.0\n",
            "unicodedata2==15.1.0\n",
            "unidecode==1.3.8\n",
            "urllib3==2.2.2\n",
            "w3lib==2.1.2\n",
            "watchdog==4.0.1\n",
            "wcwidth==0.2.13\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "werkzeug==3.0.3\n",
            "wget==3.2\n",
            "whatthepatch==1.0.2\n",
            "wheel==0.43.0\n",
            "widgetsnbextension==3.6.6\n",
            "win-inet-pton==1.1.0\n",
            "wordcloud==1.9.3\n",
            "wrapt==1.14.1\n",
            "xarray==2023.6.0\n",
            "xlwings==0.32.1\n",
            "xyzservices==2022.9.0\n",
            "yapf==0.40.2\n",
            "yarl==1.9.3\n",
            "yellowbrick==1.5\n",
            "zict==3.0.0\n",
            "zipp==3.17.0\n",
            "zope.interface==5.4.0\n",
            "zstandard==0.22.0\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "\n",
        "def list_installed_packages():\n",
        "    installed_packages = pkg_resources.working_set\n",
        "    installed_packages_list = sorted([f\"{i.key}=={i.version}\" for i in installed_packages])\n",
        "    for package in installed_packages_list:\n",
        "        print(package)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Installed Python packages:\")\n",
        "    list_installed_packages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Using cached tensorflow-2.17.0-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
            "Collecting tensorflow-intel==2.17.0 (from tensorflow)\n",
            "  Using cached tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached h5py-3.11.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting packaging (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached protobuf-4.25.4-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
            "Collecting requests<3,>=2.21.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting setuptools (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached setuptools-74.1.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting six>=1.12.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached grpcio-1.66.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting rich (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached optree-0.12.1-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached idna-3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Using cached tensorflow-2.17.0-cp312-cp312-win_amd64.whl (2.0 kB)\n",
            "Using cached tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl (385.2 MB)\n",
            "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached grpcio-1.66.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
            "Using cached h5py-3.11.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
            "Using cached keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
            "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
            "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
            "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Using cached protobuf-4.25.4-cp310-abi3-win_amd64.whl (413 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "Using cached setuptools-74.1.2-py3-none-any.whl (1.3 MB)\n",
            "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
            "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
            "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
            "Using cached idna-3.9-py3-none-any.whl (71 kB)\n",
            "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Using cached werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
            "Using cached wheel-0.44.0-py3-none-any.whl (67 kB)\n",
            "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Using cached optree-0.12.1-cp312-cp312-win_amd64.whl (267 kB)\n",
            "Using cached rich-13.8.1-py3-none-any.whl (241 kB)\n",
            "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl (17 kB)\n",
            "Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorboard-data-server, six, setuptools, pygments, protobuf, packaging, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, gast, charset-normalizer, certifi, absl-py, werkzeug, requests, optree, opt-einsum, ml-dtypes, markdown-it-py, h5py, google-pasta, astunparse, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
            "  Attempting uninstall: namex\n",
            "    Found existing installation: namex 0.0.8\n",
            "    Uninstalling namex-0.0.8:\n",
            "      Successfully uninstalled namex-0.0.8\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 18.1.1\n",
            "    Uninstalling libclang-18.1.1:\n",
            "      Successfully uninstalled libclang-18.1.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.37.1\n",
            "    Uninstalling wheel-0.37.1:\n",
            "      Successfully uninstalled wheel-0.37.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.1\n",
            "    Uninstalling urllib3-2.2.1:\n",
            "      Successfully uninstalled urllib3-2.2.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.4.0\n",
            "    Uninstalling termcolor-2.4.0:\n",
            "      Successfully uninstalled termcolor-2.4.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 69.5.1\n",
            "    Uninstalling setuptools-69.5.1:\n",
            "      Successfully uninstalled setuptools-69.5.1\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.17.2\n",
            "    Uninstalling Pygments-2.17.2:\n",
            "      Successfully uninstalled Pygments-2.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.3\n",
            "    Uninstalling protobuf-4.25.3:\n",
            "      Successfully uninstalled protobuf-4.25.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: mdurl\n",
            "    Found existing installation: mdurl 0.1.2\n",
            "    Uninstalling mdurl-0.1.2:\n",
            "      Successfully uninstalled mdurl-0.1.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.7\n",
            "    Uninstalling Markdown-3.7:\n",
            "      Successfully uninstalled Markdown-3.7\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.66.1\n",
            "    Uninstalling grpcio-1.66.1:\n",
            "      Successfully uninstalled grpcio-1.66.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.2.2\n",
            "    Uninstalling certifi-2024.2.2:\n",
            "      Successfully uninstalled certifi-2024.2.2\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 2.1.0\n",
            "    Uninstalling absl-py-2.1.0:\n",
            "      Successfully uninstalled absl-py-2.1.0\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.0.4\n",
            "    Uninstalling Werkzeug-3.0.4:\n",
            "      Successfully uninstalled Werkzeug-3.0.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: optree\n",
            "    Found existing installation: optree 0.12.1\n",
            "    Uninstalling optree-0.12.1:\n",
            "      Successfully uninstalled optree-0.12.1\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.11.0\n",
            "    Uninstalling h5py-3.11.0:\n",
            "      Successfully uninstalled h5py-3.11.0\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: astunparse\n",
            "    Found existing installation: astunparse 1.6.3\n",
            "    Uninstalling astunparse-1.6.3:\n",
            "      Successfully uninstalled astunparse-1.6.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.1\n",
            "    Uninstalling rich-13.7.1:\n",
            "      Successfully uninstalled rich-13.7.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: tensorflow-intel\n",
            "    Found existing installation: tensorflow-intel 2.17.0\n",
            "    Uninstalling tensorflow-intel-2.17.0:\n",
            "      Successfully uninstalled tensorflow-intel-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 certifi-2024.8.30 charset-normalizer-3.3.2 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.66.1 h5py-3.11.0 idna-3.9 keras-3.5.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-1.26.4 opt-einsum-3.3.0 optree-0.12.1 packaging-24.1 protobuf-4.25.4 pygments-2.18.0 requests-2.32.3 rich-13.8.1 setuptools-74.1.2 six-1.16.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-intel-2.17.0 termcolor-2.4.0 typing-extensions-4.12.2 urllib3-2.2.3 werkzeug-3.0.4 wheel-0.44.0 wrapt-1.16.0\n",
            "GPU Available:  []\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --force-reinstall tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Cek apakah GPU tersedia\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# TensorFlow akan otomatis menggunakan GPU jika tersedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Cek apakah GPU tersedia\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Pindahkan model dan data ke GPU\n",
        "model = model.to(device)\n",
        "data = data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m      4\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(\"using\", device, \"device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4UIL1x21P9rQ",
        "outputId": "ad8cd840-ff4d-4a47-f0a2-40602f819e7d"
      },
      "outputs": [],
      "source": [
        "# # Install Node.js (because tweet-harvest built using Node.js)\n",
        "# !sudo apt-get update\n",
        "# !sudo apt-get install -y ca-certificates curl gnupg\n",
        "# !sudo mkdir -p /etc/apt/keyrings\n",
        "# !curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "# !NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "# !sudo apt-get update\n",
        "# !sudo apt-get install nodejs -y\n",
        "\n",
        "# !node -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYDR51dJlVlX",
        "outputId": "a948dacf-9766-4e3b-f901-6693fd0e8ca2"
      },
      "outputs": [],
      "source": [
        "# # Crawl Data\n",
        "\n",
        "# filename = 'pemilu.csv'\n",
        "# search_keyword = '#DebatCapres until:2024-01-08 since:2024-01-06'\n",
        "# limit = 3000\n",
        "\n",
        "# !npx --yes tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" -l {limit} --token \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip uninstall matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IicyXBVj8y14",
        "outputId": "d597ed35-324e-44e1-d46a-63c28b6aa442"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"c:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"c:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"c:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\asyncio\\events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\asus.LAPTOP-P9TBK6TS.000\\AppData\\Local\\Temp\\ipykernel_36196\\741027088.py\", line 1, in <module>\n",
            "    import pandas as pd\n",
            "  File \"c:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 39, in <module>\n",
            "    from pandas.compat import (\n",
            "  File \"c:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
            "    from pandas.compat.pyarrow import (\n",
            "  File \"c:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
            "    import pyarrow as pa\n",
            "  File \"c:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:50\u001b[0m\n\u001b[0;32m     43\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not built. If you want to import \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas from the source directory, you may need to run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython setup.py build_ext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to build the C extensions first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_err\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     51\u001b[0m     get_option,\n\u001b[0;32m     52\u001b[0m     set_option,\n\u001b[0;32m     53\u001b[0m     reset_option,\n\u001b[0;32m     54\u001b[0m     describe_option,\n\u001b[0;32m     55\u001b[0m     option_context,\n\u001b[0;32m     56\u001b[0m     options,\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\pandas\\_config\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mpandas._config is considered explicitly upstream of everything else in pandas,\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mshould have no intra-pandas dependencies.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mare initialized.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect_console_encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn_copy_on_write\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dates  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     _global_config,\n\u001b[0;32m     24\u001b[0m     describe_option,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     set_option,\n\u001b[0;32m     30\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\pandas\\_config\\config.py:68\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     59\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     60\u001b[0m     Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     cast,\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     F,\n\u001b[0;32m     70\u001b[0m     T,\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "File \u001b[1;32mc:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\pandas\\_typing.py:198\u001b[0m\n\u001b[0;32m    192\u001b[0m Frequency \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseOffset\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    193\u001b[0m Axes \u001b[38;5;241m=\u001b[39m ListLike\n\u001b[0;32m    195\u001b[0m RandomState \u001b[38;5;241m=\u001b[39m Union[\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    197\u001b[0m     np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m--> 198\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mGenerator,\n\u001b[0;32m    199\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mBitGenerator,\n\u001b[0;32m    200\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState,\n\u001b[0;32m    201\u001b[0m ]\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# dtypes\u001b[39;00m\n\u001b[0;32m    204\u001b[0m NpDtype \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mdtype, type_t[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mobject\u001b[39m]]]\n",
            "File \u001b[1;32mc:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\numpy\\__init__.py:354\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m():\n\u001b[0;32m    351\u001b[0m     public_symbols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m    352\u001b[0m     public_symbols \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatrixlib\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 354\u001b[0m         \u001b[38;5;66;03m# These were moved in 1.25 and may be deprecated eventually:\u001b[39;00m\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleDeprecationWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisibleDeprecationWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplexWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTooHardError\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAxisError\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m     }\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(public_symbols)\n",
            "File \u001b[1;32mc:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\numpy\\random\\__init__.py:180\u001b[0m\n\u001b[0;32m    126\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinomial\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzipf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    177\u001b[0m ]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pickle\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _common\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _bounded_integers\n",
            "File \u001b[1;32mc:\\Users\\asus.LAPTOP-P9TBK6TS.000\\anaconda3\\Lib\\site-packages\\numpy\\random\\_pickle.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmtrand\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomState\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_philox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Philox\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pcg64\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCG64, PCG64DXSM\n",
            "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:1\u001b[0m, in \u001b[0;36minit numpy.random.mtrand\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk, re, string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn import naive_bayes\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "from nlp_id.lemmatizer import Lemmatizer\n",
        "\n",
        "\n",
        "\n",
        "from langdetect import detect\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%pip uninst\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "HvAG3hPvQDqk",
        "outputId": "f6ef9561-3efc-46ee-cbb4-9c54dabeab78"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('DataTweetFix - Copy.csv')\n",
        "\n",
        "display(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.duplicated().sum()\n",
        "\n",
        "data = data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleansing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['full_text'] = data['full_text'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "eRfDl54waHC4",
        "outputId": "1e78bce2-a154-4e57-910a-81ae483ea503"
      },
      "outputs": [],
      "source": [
        "data = data[['created_at', 'username', 'full_text']]\n",
        "\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "W8FDhBCII1bJ"
      },
      "outputs": [],
      "source": [
        "def detect_language(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "        return lang\n",
        "    except:\n",
        "        return 'unknown'\n",
        "\n",
        "data['Language'] = data['full_text'].apply(detect_language)\n",
        "\n",
        "data = data[data['Language'] == 'id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slang Word Changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_slang = pd.read_csv('kamus-alay\\colloquial-indonesian-lexicon.csv') \n",
        "\n",
        "def replace_slang(text):\n",
        "    words = text.split()\n",
        "    for i in range(len(words)):\n",
        "        slang_word = df_slang[df_slang['slang'] == words[i]]\n",
        "        if not slang_word.empty:\n",
        "            words[i] = slang_word['formal'].values[0]\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['full_text'] = data['full_text'].apply(replace_slang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLCchyCDEXuI",
        "outputId": "d1104beb-5125-429c-b8e6-2744a86e8ee3"
      },
      "outputs": [],
      "source": [
        "def remove_tweet_special(text):\n",
        "    # Menghapus tab, baris baru, dan back slice\n",
        "    text = text.replace('\\\\t', ' ').replace('\\\\n', ' ').replace('\\\\u', ' ').replace('\\\\', '')\n",
        "\n",
        "    # Menghapus karakter non-ASCII (emotikon, huruf Cina, dll.)\n",
        "    text = ''.join(char for char in text if ord(char) < 128)\n",
        "\n",
        "    # Menghapus mention, link, dan hashtag\n",
        "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
        "\n",
        "    # Menghapus URL yang tidak lengkap\n",
        "    text = text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(data):\n",
        "    # Menghapus karakter yang tidak diinginkan (_) dan karakter selain huruf\n",
        "    data['full_text'] = data['full_text'].str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()\n",
        "\n",
        "    # Mengganti multiple whitespace dengan single whitespace\n",
        "    data['full_text'] = data['full_text'].replace(\" +\", \" \", regex=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "data['full_text'] = data['full_text'].apply(remove_tweet_special)\n",
        "\n",
        "data = clean_text(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --user -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('tokenize')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return lemmatizer.lemmatize(text)\n",
        "\n",
        "def remove_numbers(text):\n",
        "    return re.sub('[0-9]+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('indonesian'))\n",
        "    return ' '.join([word for word in nltk.word_tokenize(text) if word.lower() not in stop_words])\n",
        "\n",
        "data['full_text'] = data['full_text'].apply(lambda x: x.lower())\n",
        "\n",
        "data['full_text'] = data['full_text'].apply(clean_punctuation)\n",
        "\n",
        "data['full_text'] = data['full_text'].apply(lemmatize_text)\n",
        "\n",
        "data['full_text'] = data['full_text'].apply(remove_numbers)\n",
        "\n",
        "data['full_text'] = data['full_text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "1yU08rX__j0_",
        "outputId": "123c9d30-91d4-45bb-eb72-a0c76d4ca4df"
      },
      "outputs": [],
      "source": [
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Negative and Positive Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_negative = 'InSet/negative.tsv'\n",
        "file_positive = 'Inset/positive.tsv'\n",
        "\n",
        "data_negative = pd.read_csv(file_negative, sep='\\t')\n",
        "data_positive = pd.read_csv(file_positive, sep='\\t')\n",
        "\n",
        "# Tambahkan kolom label sentimen\n",
        "data_negative['sentiment'] = 'negative'\n",
        "data_positive['sentiment'] = 'positive'\n",
        "\n",
        "# Gabungkan dataset negative dan positive menjadi satu dataset\n",
        "data_combined = pd.concat([data_negative, data_positive], ignore_index=True)\n",
        "\n",
        "# Tampilkan beberapa contoh data\n",
        "data_combined.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_weight_dict = dict(zip(data_combined['word'], data_combined['weight']))\n",
        "\n",
        "def determine_sentiment(word):\n",
        "    if word in word_weight_dict:\n",
        "        if word_weight_dict[word] < 0:\n",
        "            return 'negatif'\n",
        "        elif word_weight_dict[word] > 0:\n",
        "            return 'positif'\n",
        "    return 'netral'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Labeling data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_sentiment(data):\n",
        "    data['sentiment'] = data['full_text'].apply(lambda x: [determine_sentiment(word) for word in x.split()])\n",
        "    return data\n",
        "\n",
        "def calculate_text_weight(text):\n",
        "    total_weight = 0\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if word in word_weight_dict:\n",
        "            total_weight += word_weight_dict[word]\n",
        "    return total_weight\n",
        "\n",
        "data['text_weight'] = data['full_text'].apply(calculate_text_weight)\n",
        "\n",
        "data = test_sentiment(data)\n",
        "\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['class_sentiment'] = data['text_weight'].apply(lambda x: 'negatif' if x < 0 else ('positif' if x > 0 else 'netral'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['text_weight_label'] = data['text_weight'].apply(lambda x: 0 if x < 0 else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data[['created_at', 'full_text', 'text_weight', 'text_weight_label', 'class_sentiment']]\n",
        "\n",
        "data.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_tweets = ' '.join(data['full_text'])\n",
        "\n",
        "# Mencetak beberapa karakter pertama untuk memverifikasi\n",
        "print(\"Gabungan tweet (200 karakter pertama):\")\n",
        "print(all_tweets[:200])\n",
        "\n",
        "# Mencetak total jumlah karakter\n",
        "print(f\"\\nTotal jumlah karakter: {len(all_tweets)}\")\n",
        "\n",
        "# Mencetak jumlah kata unik\n",
        "unique_words = set(all_tweets.split())\n",
        "print(f\"Jumlah kata unik: {len(unique_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('all_tweets.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(all_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --force-reinstall pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Gabungkan semua tweet menjadi satu string\n",
        "all_tweets = ' '.join(data['full_text'])\n",
        "    \n",
        "\n",
        "# Buat visualisasi\n",
        "\n",
        "# Buat visualisasi untuk setiap sentimen\n",
        "sentiments = ['positif', 'netral', 'negatif']\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "fig.suptitle('Word Frequency berdasarkan Sentimen', fontsize=16)\n",
        "\n",
        "for i, sentiment in enumerate(sentiments):\n",
        "    tweets_by_sentiment = ' '.join(data[data['class_sentiment'] == sentiment]['full_text'])\n",
        "    \n",
        "    # Hitung frekuensi kata\n",
        "    words = tweets_by_sentiment.split()\n",
        "    word_counts = Counter(words).most_common(20)\n",
        "    \n",
        "    words, counts = zip(*word_counts)\n",
        "    \n",
        "    axes[i].bar(words, counts)\n",
        "    axes[i].set_title(sentiment.capitalize())\n",
        "    axes[i].set_xticklabels(words, rotation=90)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# Baca data\n",
        "file_negative = 'InSet/negative.tsv'\n",
        "file_positive = 'InSet/positive.tsv'\n",
        "\n",
        "data_negative = pd.read_csv(file_negative, sep='\\t')\n",
        "data_positive = pd.read_csv(file_positive, sep='\\t')\n",
        "\n",
        "# Gabungkan semua tweets\n",
        "all_tweets = ' '.join(data['full_text'])  # pastikan data sudah ada variabel `data`\n",
        "\n",
        "# Buat list kata-kata positif dan negatif\n",
        "positive_words = data_positive['word'].tolist()  # pastikan ada kolom 'word'\n",
        "negative_words = data_negative['word'].tolist()\n",
        "\n",
        "# Pisahkan kata dalam all_tweets\n",
        "words = all_tweets.split()\n",
        "\n",
        "# Hitung kemunculan setiap kata dalam tweets\n",
        "word_count = Counter(words)\n",
        "\n",
        "# Buat dictionary untuk menyimpan jumlah kata positif dan negatif\n",
        "positive_count = {word: count for word, count in word_count.items() if word in positive_words}\n",
        "negative_count = {word: count for word, count in word_count.items() if word in negative_words}\n",
        "\n",
        "# Sortir berdasarkan frekuensi tertinggi dan ambil 20 kata teratas\n",
        "positive_count = dict(sorted(positive_count.items(), key=lambda item: item[1], reverse=True)[:20])\n",
        "negative_count = dict(sorted(negative_count.items(), key=lambda item: item[1], reverse=True)[:20])\n",
        "\n",
        "# Set gaya seaborn\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Plot bar chart kata-kata positif menggunakan seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=list(positive_count.keys()), y=list(positive_count.values()), palette='Blues_d')\n",
        "plt.title('Top 20 Kata Positif di Tweets', fontsize=16)\n",
        "plt.xlabel('Kata', fontsize=12)\n",
        "plt.ylabel('Jumlah Kemunculan', fontsize=12)\n",
        "plt.xticks(rotation=90)  # Rotasi label kata agar terbaca\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot bar chart kata-kata negatif menggunakan seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=list(negative_count.keys()), y=list(negative_count.values()), palette='Reds_d')\n",
        "plt.title('Top 20 Kata Negatif di Tweets', fontsize=16)\n",
        "plt.xlabel('Kata', fontsize=12)\n",
        "plt.ylabel('Jumlah Kemunculan', fontsize=12)\n",
        "plt.xticks(rotation=90)  # Rotasi label kata agar terbaca\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['class_sentiment'].value_counts().plot(kind='bar')\n",
        "\n",
        "plt.xticks(rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(data['text_weight'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Histogram dari Nilai Bobot dalam Kolom \"text_weight\"')\n",
        "plt.xlabel('Nilai Bobot')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "Train_X, Test_X, Train_Y, Test_Y = train_test_split(data['full_text'], data['class_sentiment'], test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=5000, oov_token='x', filters='!\"#$%&()*+,-./:;<=>@[\\]^_`{|}~ ')\n",
        "\n",
        "tokenizer.fit_on_texts(data['full_text'])\n",
        "\n",
        "# Konversi teks menjadi sequence\n",
        "data_sequences = tokenizer.texts_to_sequences(data['full_text'])\n",
        "\n",
        "padded_train = pad_sequences(data_sequences)\n",
        "\n",
        "# Menampilkan hasil sequence\n",
        "print(padded_train)\n",
        "\n",
        "# tokenizer.fit_on_texts(Train_X)\n",
        "# tokenizer.fit_on_texts(Test_X)\n",
        "\n",
        "# sekuens_train = tokenizer.texts_to_sequences(Train_X)\n",
        "# sekuens_test = tokenizer.texts_to_sequences(Test_X)\n",
        "\n",
        "# padded_train = pad_sequences(sekuens_train)\n",
        "# padded_test = pad_sequences(sekuens_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Jumlah data dalam train set: {len(Train_X)}')\n",
        "print(f'Jumlah data dalam validation set: {len(Test_X)}')\n",
        "print(f'Jumlah data dalam train set: {len(Train_Y)}')\n",
        "print(f'Jumlah data dalam validation set: {len(Test_Y)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "Encoder = LabelEncoder()\n",
        "\n",
        "Train_Y = Encoder.fit_transform(Train_Y)\n",
        "Test_Y = Encoder.fit_transform(Test_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(data['full_text'])\n",
        "\n",
        "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
        "Train_X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "X = data['full_text']\n",
        "y = data['class_sentiment']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "X_train_Tfidf = Tfidf_vect.fit_transform(X_train)\n",
        "X_test_Tfidf = Tfidf_vect.transform(X_test)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_Tfidf, y_train)\n",
        "\n",
        "# Definisikan model dan parameter grid\n",
        "models = {\n",
        "    'Naive Bayes': naive_bayes.MultinomialNB(),\n",
        "    'SVM': SVC(probability=True),\n",
        "    'Random Forest': RandomForestClassifier()\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'Naive Bayes': {'alpha': [0.1, 0.5, 1.0]},\n",
        "    'SVM': {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']},\n",
        "    'Random Forest': {'n_estimators': [100, 200], 'max_depth': [10, 20, None]}\n",
        "}\n",
        "\n",
        "# Grid Search dan evaluasi\n",
        "best_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "    best_models[name] = grid_search.best_estimator_\n",
        "    \n",
        "    y_pred = best_models[name].predict(X_test_Tfidf)\n",
        "    print(f\"\\n{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Ensemble voting\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[(name, model) for name, model in best_models.items()],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_voting = voting_clf.predict(X_test_Tfidf)\n",
        "print(\"\\nVoting Classifier Accuracy:\", accuracy_score(y_test, y_pred_voting))\n",
        "print(classification_report(y_test, y_pred_voting))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Klasifikasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(Train_X_Tfidf, Train_Y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "\n",
        "print(confusion_matrix(predictions_NB,Test_Y))\n",
        "print (\"Classification: \",classification_report(Test_Y,predictions_NB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Naive Bayes Accuracy Score -> \", accuracy_score(predictions_NB, Test_Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(Test_Y, predictions_NB)\n",
        "cm = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=['Positif', 'Netral', 'Negatif'])\n",
        "cm.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(Naive, 'ModelAnalisisSentimen.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.load('ModelAnalisisSentimen.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = Naive.predict(Test_X_Tfidf)\n",
        "predictions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
